{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138daba0-229a-4283-bf12-1f83d875f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'conda-forge'\n"
     ]
    }
   ],
   "source": [
    "pip install -c conda-forge geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d11d79d-19fa-48a3-bc99-a04abfc9927c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'geopandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgeopandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpd\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'geopandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import des bibliothèques supplémentaires\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro, levene\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75576326-3aff-4463-986c-8d4bc9adbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"###############################################################\")\n",
    "print(\"###  1. Chargement et Préparation des Données                 ###\")\n",
    "print(\"###############################################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf65a187-3618-41ec-85b9-742dad59a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du répertoire de travail\n",
    "import os\n",
    "os.chdir(\"D:/Mémoire master fidel/Thème_2/WQI/Data\")\n",
    "\n",
    "# Chemins des fichiers\n",
    "shp_path = \"D:/Mémoire master fidel/Thème_2/WQI/SIG/Shp/Lake_Aheme.shp\"\n",
    "\n",
    "# Importation des données\n",
    "data_aheme = pd.read_excel(\"Data_Aheme.xlsx\", sheet_name=\"Data_Aheme\")\n",
    "\n",
    "# Création d'une copie du dataframe\n",
    "df_copy = data_aheme.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37ebf72-5b62-427b-acdd-fedac7d09235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Traitement des dates et heures\n",
    "df_copy['Date'] = pd.to_datetime(df_copy['Date'], format='mixed', dayfirst=True)\n",
    "df_copy['Heure'] = df_copy['Heure'].astype(str)\n",
    "df_copy['Year_month'] = df_copy['Date'].dt.strftime('%Y-%m')\n",
    "\n",
    "# 2) Imputation des valeurs manquantes par la moyenne par groupe\n",
    "def impute_missing(group):\n",
    "    numeric_cols = group.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        group[col] = group[col].fillna(group[col].mean())\n",
    "    return group\n",
    "\n",
    "df_clean = df_copy.groupby(['Saison', 'Stations']).apply(impute_missing).reset_index(drop=True)\n",
    "\n",
    "# 3) Sélection et renommage des colonnes\n",
    "cols_to_keep = [col for col in df_clean.columns if not col.startswith('cleaned_')]\n",
    "df_clean = df_clean[cols_to_keep]\n",
    "\n",
    "# Supprimer les colonnes spécifiques\n",
    "cols_to_drop = ['Saison', 'Code', 'Redox', 'N_NO3']\n",
    "df_clean = df_clean.drop(columns=[col for col in cols_to_drop if col in df_clean.columns])\n",
    "\n",
    "# 4) Conversion des types\n",
    "df_clean['Stations'] = df_clean['Stations'].astype('category')\n",
    "df_clean['Date'] = pd.to_datetime(df_clean['Date'])\n",
    "\n",
    "# 5) Sauvegarde du dataframe nettoyé\n",
    "df_clean.to_csv(\"df_aheme.csv\", index=False)\n",
    "\n",
    "# 6) Création d'un dataframe numérique seulement\n",
    "df_num = df_clean.select_dtypes(include=[np.number])\n",
    "cols_to_exclude = ['X', 'Y', 'Long', 'Lat']\n",
    "df_num = df_num.drop(columns=[col for col in cols_to_exclude if col in df_num.columns])\n",
    "\n",
    "print(\"\\n\\n###############################################################\")\n",
    "print(\"###      Analyse descriptive pour chaque paramètre             ###\")\n",
    "print(\"###############################################################\\n\")\n",
    "\n",
    "# Analyse descriptive\n",
    "desc = df_num.describe()\n",
    "desc.to_csv(\"resultats_descriptifs.csv\")\n",
    "\n",
    "def stats_desc(variable):\n",
    "    \"\"\"Fonction pour calculer les statistiques descriptives\"\"\"\n",
    "    sd_value = variable.std()\n",
    "    cv_value = sd_value / variable.mean() if variable.mean() != 0 else np.nan\n",
    "    \n",
    "    # Test de Shapiro-Wilk\n",
    "    try:\n",
    "        shapiro_stat, shapiro_p = shapiro(variable.dropna())\n",
    "    except:\n",
    "        shapiro_stat, shapiro_p = np.nan, np.nan\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Ecart_type': round(sd_value, 2),\n",
    "        'Coeff_variation': round(cv_value, 2) if not np.isnan(cv_value) else np.nan,\n",
    "        'W_Shapiro': round(shapiro_stat, 2) if not np.isnan(shapiro_stat) else np.nan,\n",
    "        'P_Shapiro': shapiro_p\n",
    "    })\n",
    "\n",
    "# Application sur toutes les colonnes numériques\n",
    "resultats = df_num.apply(stats_desc)\n",
    "resultats['Variables'] = df_num.columns\n",
    "print(resultats[['Variables', 'Ecart_type', 'Coeff_variation', 'W_Shapiro', 'P_Shapiro']])\n",
    "\n",
    "# Description détaillée avec scipy\n",
    "from scipy.stats import describe\n",
    "print(\"\\nDescription détaillée:\")\n",
    "for col in df_num.columns:\n",
    "    desc_stats = describe(df_num[col].dropna())\n",
    "    print(f\"{col}: n={desc_stats.nobs}, min={desc_stats.minmax[0]:.2f}, max={desc_stats.minmax[1]:.2f}, mean={desc_stats.mean:.2f}\")\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###  Calculer la matrice de corrélation pour tous les paramètres   ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# Matrice de corrélation\n",
    "cor_matrix = df_num.corr(method='pearson').round(2)\n",
    "cor_matrix.to_csv(\"resultats_cormatrix.csv\")\n",
    "\n",
    "# Visualisation de la matrice de corrélation\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cor_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8})\n",
    "plt.title('Matrice de Corrélation')\n",
    "plt.tight_layout()\n",
    "plt.savefig('grap_correlation.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Matrice de corrélation avec nombres\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cor_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={\"shrink\": .8}, annot_kws={\"size\": 8})\n",
    "plt.title('Matrice de Corrélation (Valeurs)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('grap_correlationnum.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Test de significativité des corrélations\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def calculate_pvalues(df):\n",
    "    \"\"\"Calcule les p-values pour la matrice de corrélation\"\"\"\n",
    "    df = df.dropna()._get_numeric_data()\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            pvalues[r][c] = round(pearsonr(df[r], df[c])[1], 3)\n",
    "    return pvalues\n",
    "\n",
    "pvalues_matrix = calculate_pvalues(df_num)\n",
    "print(\"P-values de la corrélation:\")\n",
    "print(pvalues_matrix)\n",
    "\n",
    "# Diagramme de paires\n",
    "sns.pairplot(df_num)\n",
    "plt.savefig('pairplot.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###                      ANALYSE EN COMPOSANTES PRINCIPALES (ACP)           ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# ACP\n",
    "df_scale = StandardScaler().fit_transform(df_num)\n",
    "pca = PCA()\n",
    "pca_result = pca.fit_transform(df_scale)\n",
    "\n",
    "# Résumé de l'ACP\n",
    "print(\"Variance expliquée par chaque composante:\")\n",
    "print(pd.DataFrame({\n",
    "    'Composante': range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "    'Variance Expliquée': pca.explained_variance_ratio_,\n",
    "    'Variance Cumulée': np.cumsum(pca.explained_variance_ratio_)\n",
    "}))\n",
    "\n",
    "# Biplot\n",
    "def plot_biplot(pca_result, features, feature_names):\n",
    "    \"\"\"Fonction pour créer un biplot\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot des points\n",
    "    scatter = plt.scatter(pca_result[:, 0], pca_result[:, 1], alpha=0.7)\n",
    "    \n",
    "    # Plot des vecteurs des variables\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        plt.arrow(0, 0, pca.components_[0, i], pca.components_[1, i], \n",
    "                 color='r', alpha=0.5, head_width=0.05)\n",
    "        plt.text(pca.components_[0, i]*1.15, pca.components_[1, i]*1.15, \n",
    "                feature, color='r', ha='center', va='center')\n",
    "    \n",
    "    plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})')\n",
    "    plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})')\n",
    "    plt.title('Biplot ACP')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='--', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('pca_biplot.png', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "plot_biplot(pca_result, pca.components_, df_num.columns)\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###                 ANALYSE DISCRIMINANTE LINEAIRE (LDA)                ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# Préparation des données pour LDA\n",
    "X_lda = df_clean[['Temperature', 'Conductivity', 'Turbidity', 'Salinity', \n",
    "                  'O2', 'Chl_a', 'pH', 'NT', 'PT']].dropna()\n",
    "y_lda = df_clean.loc[X_lda.index, 'Stations']\n",
    "\n",
    "# LDA\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda_result = lda.fit(X_lda, y_lda)\n",
    "\n",
    "# Projections\n",
    "lda_pred = lda.transform(X_lda)\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(lda_pred[:, 0], lda_pred[:, 1], c=pd.Categorical(y_lda).codes, cmap='viridis')\n",
    "plt.xlabel('LD1')\n",
    "plt.ylabel('LD2')\n",
    "plt.title('Séparation des stations sur LD1 et LD2')\n",
    "plt.legend(handles=scatter.legend_elements()[0], labels=list(y_lda.unique()))\n",
    "plt.tight_layout()\n",
    "plt.savefig('lda_plot.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Coefficients LDA\n",
    "coefficients = pd.DataFrame(lda.scaling_, index=X_lda.columns, columns=[f'LD{i+1}' for i in range(lda.scaling_.shape[1])])\n",
    "print(\"Coefficients LDA:\")\n",
    "print(coefficients)\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###                         CLUSTERING (K-MEANS)                          ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# K-means clustering\n",
    "kmeans = KMeans(n_clusters=3, random_state=12, n_init=10)\n",
    "kmeans_result = kmeans.fit_predict(df_num.dropna())\n",
    "\n",
    "# Ajout des clusters au dataframe\n",
    "df_clustered = df_num.dropna().copy()\n",
    "df_clustered['cluster'] = kmeans_result\n",
    "\n",
    "# Visualisation\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(df_clustered.iloc[:, 0], df_clustered.iloc[:, 1], c=df_clustered['cluster'], cmap='viridis')\n",
    "plt.xlabel(df_clustered.columns[0])\n",
    "plt.ylabel(df_clustered.columns[1])\n",
    "plt.title('Clustering K-means')\n",
    "plt.colorbar(label='Cluster')\n",
    "plt.tight_layout()\n",
    "plt.savefig('kmeans_clustering.png', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Résultats K-means:\")\n",
    "print(f\"Centroïdes: {kmeans.cluster_centers_}\")\n",
    "print(f\"Inertie: {kmeans.inertia_:.2f}\")\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"########                 ANALYSE OF VARIANCE (ANOVA)                   ########\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# Tests de normalité\n",
    "def normality_test_wrapper(data):\n",
    "    \"\"\"Applique le test de Shapiro-Wilk sur chaque colonne\"\"\"\n",
    "    results = []\n",
    "    for col in data.columns:\n",
    "        if data[col].notna().sum() > 3:  # Minimum 3 observations pour le test\n",
    "            stat, p_value = shapiro(data[col].dropna())\n",
    "            results.append({\n",
    "                'Variable': col,\n",
    "                'W_Shapiro': round(stat, 2),\n",
    "                'P_Shapiro': p_value\n",
    "            })\n",
    "        else:\n",
    "            results.append({\n",
    "                'Variable': col,\n",
    "                'W_Shapiro': np.nan,\n",
    "                'P_Shapiro': np.nan\n",
    "            })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "normality_results = normality_test_wrapper(df_num)\n",
    "print(\"Tests de normalité (Shapiro-Wilk):\")\n",
    "print(normality_results)\n",
    "\n",
    "# Préparation des données pour ANOVA\n",
    "param_cols = [\"Temperature\", \"Conductivity\", \"Salinity\", \"O2\", \"Saturation\", \"pH\",\n",
    "              \"Transparence\", \"Turbidity\", \"Chl_a\", \"N_NO2\", \"N_NH4\", \"P_PO4\", \"PT\", \"NT\"]\n",
    "\n",
    "long_df = df_clean.melt(\n",
    "    id_vars=['Date', 'Stations'], \n",
    "    value_vars=param_cols, \n",
    "    var_name='parameter', \n",
    "    value_name='value'\n",
    ").dropna()\n",
    "\n",
    "long_df['period'] = long_df['Date'].dt.to_period('M')\n",
    "long_df['period_lab'] = long_df['period'].astype(str)\n",
    "\n",
    "# Test de Levene pour l'homogénéité des variances\n",
    "levene_stat, levene_p = levene(*[group['value'].values for name, group in long_df.groupby('parameter')])\n",
    "print(f\"\\nTest de Levene: statistic={levene_stat:.3f}, p-value={levene_p:.3f}\")\n",
    "\n",
    "# ANOVA\n",
    "formula = 'value ~ C(Stations) + C(period_lab) + parameter + C(Stations):C(period_lab)'\n",
    "model = ols(formula, data=long_df).fit()\n",
    "anova_results = sm.stats.anova_lm(model, typ=2)\n",
    "print(\"\\nRésultats ANOVA:\")\n",
    "print(anova_results)\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###                    ANALYSE SPATIO-TEMPORELLE                     ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# Chargement des données spatiales\n",
    "study_area = gpd.read_file(shp_path)\n",
    "pts = gpd.GeoDataFrame(\n",
    "    df_clean, \n",
    "    geometry=gpd.points_from_xy(df_clean.Long, df_clean.Lat),\n",
    "    crs='EPSG:32631'\n",
    ")\n",
    "\n",
    "# Reprojection si nécessaire\n",
    "if study_area.crs != pts.crs:\n",
    "    study_area = study_area.to_crs(pts.crs)\n",
    "\n",
    "# Filtrage des points dans la zone d'étude\n",
    "pts = gpd.sjoin(pts, study_area, predicate='within')\n",
    "\n",
    "# Préparation des données en format long pour l'analyse spatiale\n",
    "param_cols_spatial = [\"Temperature\", \"Conductivity\", \"Salinity\", \"O2\", \"Saturation\", \"pH\",\n",
    "                     \"Turbidity\", \"Transparence\", \"Chl_a\", \"N_NO2\", \"N_NH4\", \"P_PO4\", \"NT\", \"PT\"]\n",
    "\n",
    "long_spatial = pts.melt(\n",
    "    id_vars=['Date', 'Stations', 'geometry'], \n",
    "    value_vars=param_cols_spatial, \n",
    "    var_name='parameter', \n",
    "    value_name='value'\n",
    ").dropna()\n",
    "\n",
    "long_spatial['period'] = long_spatial['Date'].dt.to_period('M')\n",
    "long_spatial['period_lab'] = long_spatial['period'].astype(str)\n",
    "\n",
    "# Calcul des limites globales pour chaque paramètre\n",
    "param_limits = long_spatial.groupby('parameter')['value'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "def plot_param_points(data_long, area_sf, param, date_from=None, date_to=None, \n",
    "                     transform=None, ncols=7, point_size=6, param_limits=None):\n",
    "    \"\"\"Fonction pour créer des cartes de points par paramètre\"\"\"\n",
    "    \n",
    "    d = data_long[data_long['parameter'] == param].copy()\n",
    "    \n",
    "    if date_from:\n",
    "        d = d[d['Date'] >= pd.to_datetime(date_from)]\n",
    "    if date_to:\n",
    "        d = d[d['Date'] <= pd.to_datetime(date_to)]\n",
    "    \n",
    "    if transform == \"log1p\":\n",
    "        d['value_plot'] = np.log1p(np.maximum(d['value'], 0))\n",
    "        color_label = f\"{param} (log1p)\"\n",
    "    else:\n",
    "        d['value_plot'] = d['value']\n",
    "        color_label = param\n",
    "    \n",
    "    # Obtenir les limites pour ce paramètre\n",
    "    param_min = param_limits[param_limits['parameter'] == param]['min'].iloc[0]\n",
    "    param_max = param_limits[param_limits['parameter'] == param]['max'].iloc[0]\n",
    "    \n",
    "    # Création du graphique\n",
    "    unique_periods = d['period_lab'].unique()\n",
    "    nrows = int(np.ceil(len(unique_periods) / ncols))\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*3, nrows*3))\n",
    "    axes = axes.flatten() if nrows > 1 else [axes]\n",
    "    \n",
    "    for i, period in enumerate(unique_periods):\n",
    "        if i < len(axes):\n",
    "            period_data = d[d['period_lab'] == period]\n",
    "            \n",
    "            # Plot de la zone d'étude\n",
    "            area_sf.boundary.plot(ax=axes[i], color='grey40', linewidth=0.4)\n",
    "            \n",
    "            if not period_data.empty:\n",
    "                # Plot des points\n",
    "                scatter = period_data.plot(ax=axes[i], column='value_plot', \n",
    "                                         markersize=point_size, alpha=0.9, \n",
    "                                         legend=False, cmap='viridis',\n",
    "                                         vmin=param_min, vmax=param_max)\n",
    "            \n",
    "            axes[i].set_title(period)\n",
    "            axes[i].set_axis_off()\n",
    "    \n",
    "    # Cacher les axes vides\n",
    "    for i in range(len(unique_periods), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Ajouter une barre de couleur\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                              norm=plt.Normalize(vmin=param_min, vmax=param_max))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=axes, shrink=0.6)\n",
    "    cbar.set_label(color_label)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Exemple d'utilisation pour la température\n",
    "fig_temp = plot_param_points(long_spatial, study_area, \"Temperature\", \n",
    "                            ncols=7, param_limits=param_limits)\n",
    "plt.savefig(\"maps_temperature_points.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Interpolation IDW\n",
    "def idw_interpolation(points, target_points, power=2):\n",
    "    \"\"\"Interpolation par Inverse Distance Weighting\"\"\"\n",
    "    distances = cdist(target_points, points)\n",
    "    weights = 1 / (distances ** power)\n",
    "    weights[weights == np.inf] = 1  # Gérer les distances nulles\n",
    "    weights = weights / weights.sum(axis=1, keepdims=True)\n",
    "    return weights\n",
    "\n",
    "def idw_param_map_sf(data_long, area_sf, param, cellsize=100, date_from=None, \n",
    "                    date_to=None, idp=2, nmax=12, crs_proj=3857, ncol=3, global_limits=None):\n",
    "    \"\"\"Fonction pour créer des cartes d'interpolation IDW\"\"\"\n",
    "    \n",
    "    d = data_long[data_long['parameter'] == param].copy()\n",
    "    \n",
    "    if date_from:\n",
    "        d = d[d['Date'] >= pd.to_datetime(date_from)]\n",
    "    if date_to:\n",
    "        d = d[d['Date'] <= pd.to_datetime(date_to)]\n",
    "    \n",
    "    # Reprojection\n",
    "    area_prj = area_sf.to_crs(crs_proj)\n",
    "    d_prj = d.to_crs(crs_proj)\n",
    "    \n",
    "    periods = sorted(d_prj['period_lab'].unique())\n",
    "    \n",
    "    # Limites globales\n",
    "    param_min = global_limits[global_limits['parameter'] == param]['min'].iloc[0]\n",
    "    param_max = global_limits[global_limits['parameter'] == param]['max'].iloc[0]\n",
    "    \n",
    "    # Création de la grille\n",
    "    bounds = area_prj.total_bounds\n",
    "    x = np.arange(bounds[0], bounds[2], cellsize)\n",
    "    y = np.arange(bounds[1], bounds[3], cellsize)\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "    \n",
    "    # Filtrer les points dans la zone d'étude\n",
    "    grid_gdf = gpd.GeoDataFrame(\n",
    "        geometry=gpd.points_from_xy(grid_points[:, 0], grid_points[:, 1]),\n",
    "        crs=area_prj.crs\n",
    "    )\n",
    "    grid_gdf = gpd.sjoin(grid_gdf, area_prj, predicate='within')\n",
    "    \n",
    "    nrows = int(np.ceil(len(periods) / ncol))\n",
    "    fig, axes = plt.subplots(nrows, ncol, figsize=(ncol*4, nrows*4))\n",
    "    axes = axes.flatten() if nrows > 1 else [axes]\n",
    "    \n",
    "    for i, period in enumerate(periods):\n",
    "        if i < len(axes):\n",
    "            period_data = d_prj[d_prj['period_lab'] == period]\n",
    "            \n",
    "            if len(period_data) >= 3:\n",
    "                # Points d'observation\n",
    "                obs_points = np.array([point.coords[0] for point in period_data.geometry])\n",
    "                obs_values = period_data['value'].values\n",
    "                \n",
    "                # Points cibles\n",
    "                target_points = np.array([point.coords[0] for point in grid_gdf.geometry])\n",
    "                \n",
    "                # Interpolation IDW\n",
    "                weights = idw_interpolation(obs_points, target_points, power=idp)\n",
    "                pred_values = weights.dot(obs_values)\n",
    "                \n",
    "                # Création de la grille de prédiction\n",
    "                grid_gdf_period = grid_gdf.copy()\n",
    "                grid_gdf_period['pred'] = pred_values\n",
    "                \n",
    "                # Plot\n",
    "                area_prj.boundary.plot(ax=axes[i], color='grey30', linewidth=0.3)\n",
    "                grid_gdf_period.plot(ax=axes[i], column='pred', cmap='viridis', \n",
    "                                   vmin=param_min, vmax=param_max, legend=False)\n",
    "            \n",
    "            axes[i].set_title(period)\n",
    "            axes[i].set_axis_off()\n",
    "    \n",
    "    # Cacher les axes vides\n",
    "    for i in range(len(periods), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    # Barre de couleur\n",
    "    sm = plt.cm.ScalarMappable(cmap='viridis', \n",
    "                              norm=plt.Normalize(vmin=param_min, vmax=param_max))\n",
    "    sm._A = []\n",
    "    cbar = fig.colorbar(sm, ax=axes, shrink=0.6)\n",
    "    cbar.set_label(param)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Application de l'interpolation IDW\n",
    "global_limits_spatial = long_spatial.groupby('parameter')['value'].agg(['min', 'max']).reset_index()\n",
    "\n",
    "# Température\n",
    "fig_temp_idw = idw_param_map_sf(long_spatial, study_area, \"Temperature\",\n",
    "                               date_from=\"2023-01-01\", date_to=\"2024-12-31\",\n",
    "                               cellsize=100, ncol=7, global_limits=global_limits_spatial)\n",
    "plt.savefig(\"temperature_idw_2024.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\n\\n###################################################################\")\n",
    "print(\"###                    ANALYSE STATISTIQUE COMPLÈTE                    ###\")\n",
    "print(\"####################################################################\\n\")\n",
    "\n",
    "# Statistiques descriptives par paramètre et période\n",
    "stat_desc_period = long_df.groupby(['parameter', 'period_lab']).agg({\n",
    "    'value': ['mean', 'std', 'min', 'max', 'count']\n",
    "}).round(2)\n",
    "\n",
    "print(\"Statistiques descriptives par paramètre et période:\")\n",
    "print(stat_desc_period)\n",
    "\n",
    "# Graphique des tendances temporelles\n",
    "plt.figure(figsize=(12, 8))\n",
    "for station in df_clean['Stations'].unique():\n",
    "    station_data = df_clean[df_clean['Stations'] == station]\n",
    "    plt.plot(station_data['Date'], station_data['Temperature'], \n",
    "             label=station, marker='o', markersize=3)\n",
    "\n",
    "plt.title('Tendance de la température dans le temps')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Température')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('temperature_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Histogramme de la température\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df_clean['Temperature'].dropna(), bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribution de la température')\n",
    "plt.xlabel('Température')\n",
    "plt.ylabel('Fréquence')\n",
    "plt.tight_layout()\n",
    "plt.savefig('temperature_histogram.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Scatter plot température vs salinité\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_clean['Temperature'], df_clean['Salinity'], alpha=0.6)\n",
    "plt.title('Température vs Salinité')\n",
    "plt.xlabel('Température')\n",
    "plt.ylabel('Salinité')\n",
    "plt.tight_layout()\n",
    "plt.savefig('temp_vs_salinity.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "# Carte des stations\n",
    "plt.figure(figsize=(10, 8))\n",
    "study_area.boundary.plot(color='black', linewidth=1)\n",
    "pts.plot(ax=plt.gca(), column='Stations', legend=True, markersize=50)\n",
    "plt.title('Localisation des Stations')\n",
    "plt.xlabel('Longitude')\n",
    "plt.ylabel('Latitude')\n",
    "plt.tight_layout()\n",
    "plt.savefig('stations_map.png', dpi=300, bbox_inches='tight')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nAnalyse terminée avec succès!\")\n",
    "print(\"Tous les graphiques ont été sauvegardés dans le répertoire de travail.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
